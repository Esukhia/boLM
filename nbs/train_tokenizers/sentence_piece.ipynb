{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n",
      "Requirement already satisfied: nbd-colab in /usr/local/lib/python3.6/dist-packages (0.0.10)\n",
      "Requirement already satisfied: fastcore in /usr/local/lib/python3.6/dist-packages (from nbd-colab) (0.1.16)\n",
      "Requirement already satisfied: nbdev in /usr/local/lib/python3.6/dist-packages (from nbd-colab) (0.2.17)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fastcore->nbd-colab) (1.18.2)\n",
      "Requirement already satisfied: dataclasses>='0.7'; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastcore->nbd-colab) (0.7)\n",
      "Requirement already satisfied: fastscript in /usr/local/lib/python3.6/dist-packages (from nbdev->nbd-colab) (0.1.4)\n",
      "Requirement already satisfied: nbformat>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from nbdev->nbd-colab) (5.0.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from nbdev->nbd-colab) (3.13)\n",
      "Requirement already satisfied: nbconvert>=5.6.1 in /usr/local/lib/python3.6/dist-packages (from nbdev->nbd-colab) (5.6.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from nbdev->nbd-colab) (20.3)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev->nbd-colab) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev->nbd-colab) (2.6.0)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev->nbd-colab) (4.6.3)\n",
      "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4.0->nbdev->nbd-colab) (4.3.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.6.1->nbdev->nbd-colab) (2.11.1)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.6.1->nbdev->nbd-colab) (0.4.4)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.6.1->nbdev->nbd-colab) (2.1.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.6.1->nbdev->nbd-colab) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.6.1->nbdev->nbd-colab) (0.3)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.6.1->nbdev->nbd-colab) (0.6.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.6.1->nbdev->nbd-colab) (3.1.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert>=5.6.1->nbdev->nbd-colab) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->nbdev->nbd-colab) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->nbdev->nbd-colab) (1.12.0)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1->nbformat>=4.4.0->nbdev->nbd-colab) (4.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert>=5.6.1->nbdev->nbd-colab) (1.1.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert>=5.6.1->nbdev->nbd-colab) (0.5.1)\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece\n",
    "\n",
    "!pip install nbd-colab\n",
    "\n",
    "from nbd_colab import *\n",
    "\n",
    "drive_setup()\n",
    "home_dir()\n",
    "\n",
    "repo_name = 'bonltk'\n",
    "change_dir(f'/content/drive/My Drive/Notebooks/Esukhia/{repo_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('.bonltk/data/corpora/esukhia-dergey-katen')\n",
    "corpus_fn = data_path/f'all-{data_path.name}.txt'\n",
    "tokenized_corpus_fn = data_path/f'all-{data_path.name}-tokenized.txt'\n",
    "\n",
    "models_path = Path('.bonltk/models/tokenizers')\n",
    "models_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undo_tokenization(text):\n",
    "    for f, t in [(' ', ''), ('_', ' ')]:\n",
    "        text = text.replace(f, t)\n",
    "    return text\n",
    "\n",
    "def combine_files(source_path, out_fn, pretokenized=False, verbose=False):\n",
    "    corpus = ''\n",
    "    n_sentences = 0\n",
    "    if out_fn.is_file(): return out_fn\n",
    "    for path in tqdm(list(source_path.iterdir())):\n",
    "        if path.is_file(): continue\n",
    "        sentences = ''\n",
    "        for fn in tqdm(list(path.iterdir())):\n",
    "            text = fn.read_text()\n",
    "            if not pretokenized:\n",
    "                text = undo_tokenization(text)\n",
    "            if verbose:\n",
    "                n_sentences += text.count('\\n')\n",
    "            corpus += text\n",
    "    if verbose:\n",
    "        print('[INFO] No. sentences the courpus contains:', n_sentences)\n",
    "    out_fn.write_text(corpus)\n",
    "    return out_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b6382f4d27422f9f264446c8cc52b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a19eae557784ff29254311838950edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=103), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c323a03612be41dbb4e770e1df1188d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=213), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[INFO] No. sentences the courpus contains: 2990486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('.bonltk/data/corpora/esukhia-dergey-katen/all-esukhia-dergey-katen.txt')"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_files(data_path, corpus_fn, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "༄༅། །འདུལ་བ་ང་བཞུགས་སོ།།༄༅༅། །\n",
      "འདུལ་བ་གཞི། བམ་པོ་བརྒྱད་བཅུ་གསུམ་པ། གཟིགས་ནས་\n",
      "ཀྱང་ཁ་ལོ་སྒྱུར་བ་ལ་གསུངས་པ། ཁ་ལོ་སྒྱུར་བ་མི་སྐྱ་བོ་རྦབ་རྦབ་པོ་རིད་པ་ཉམ་ཆུང་བ་དབང་པོ་ཉམས་པ། སྐྱེ་བོ་མང་པོའི་མིག་གིས་ཀྱང་ལྟ་མི་ཕོད་པ་འདི་ཅི་ཡིན།\n",
      "ལྷ་འདི་ནི་བྲོ་འཚལ་བ་ཞེས་བགྱིའོ། །\n",
      "ཁ་ལོ་སྒྱུར་བ་ནད་ཀྱིས་ཐེབས་པ་ཞེས་བྱ་བ་འདི་ཅི་ཡིན།\n",
      "ལྷ་མི་འདི་གང་འདིས་འགུམ་པར་འགྱུར་བའི་གནས་དེ་མཆིས་ཏེ།\n",
      "ལྷ་འདི་ནི་བྲོས་ཐེབས་པ་ཞེས་བགྱིའོ། །\n",
      "ཁ་ལོ་སྒྱུར་བ་ང་ཡང་ནད་ཀྱི་ཆོས་དང་ནད་ཀྱི་ཆོས་ཉིད་ལས་མ་འདས་སམ། ལྷ་ཡང་སྙུན་གྱི་ཆོས་དང་སྙུན་གྱི་ཆོས་ཉིད་ལས་མ་འདས་སོ། །\n",
      "ཁ་ལོ་སྒྱུར་བ་དེའི་ཕྱིར་ཤིང་རྟ་ཕྱིར་སྒྱུར་ཅིག་ཕོ་བྲང་ཉིད་དུ་འདོང་ངོ་། །\n",
      "འདི་ལྟར་ང་ཕོ་བྲང་ཉིད་དུ་ཕྱིན་ནས་ང་ནད་ལས་མ་འདས་ཞེས་གྲགས་པའི་དོན་དེ་ཉིད་བསམ་པར་བྱའོ། །\n"
     ]
    }
   ],
   "source": [
    "!head {corpus_fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9470cf8f53e42259a73ae613a5da152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33d33496c7c4001b5e635be28444fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=103), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128a6a7f75bd4a7dbb4acef4a6df8a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=213), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('.bonltk/data/corpora/esukhia-dergey-katen/all-esukhia-dergey-katen-tokenized.txt')"
      ]
     },
     "execution_count": null,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_files(data_path, tokenized_corpus_fn, pretokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "དགའ་བ་ ནི་ ཤིན་ཏུ་ སྦྱངས་པ་ དང་ ལྡན་པ ར་ བྱེད་ དེ །\n",
      "ཡིད་ དགའ་བ འི་ ལུས་ ཤིན་ཏུ་ སྦྱངས་པ ར་ འགྱུར་ རོ་ ཞེས་ འབྱུང་བ འི་ ཕྱིར་ རོ །_། ལུས་ མཉེན་པ ར་ བྱེད་པ་ ཞེས་ བྱ་བ་ ནི་ལ ས་ སུ་ རུང་བ ར་ བྱེད་པ འོ །_།\n",
      "དེ་ཉིད་ ཀྱི་ ཕྱིར་ ཞེས་ བྱ་བ་ ནི་ ལུས་ མཉེན་པ ར་ བྱེད་པ འི་ ཕྱིར་ ཞེས་ བྱ་བ འི་ ཐ་ཚིག་ གོ །_།\n",
      "བཞི་པ་ ན་ མེད་ མི་ གཡོ་ ཕྱིར །_། ཞེས་ བྱ་བ་ ནི་ མི་ འགུལ་ ཞེས་ བྱ་བ འི་ དོན་ ཏོ །_། ནང་ གི་ སྐྱོན་ དང་ བྲལ་བ འི་ ཕྱིར །_། ཞེས་ བྱ་བ་ ལ །_ རྟོག་ དང་ དཔྱོད་ དང་ དབུགས་ དག་ དང་ །_། བདེ་བ་ ལ་སོགས་ བཞི་ ཡིན་ ནོ །_། ཞེས་ རྟོག་པ་ དང་ དཔྱོད་པ་ ལ་སོགས་པ་ རྣམས་ སྐྱོན་ དུ་ འཆད་པ ར་ འགྱུར་ རོ །_། དེ་དག་ གིས་ གཟུགས་ མེད་པ་ དག་ ཏུ་ འཇུག་པ ར་ མི་ ནུས་ ལ་ ཞེས་ བྱ་བ་ ནི་ གནས་ གཙང་མ་ རྣམས་ ལྷག་མཐོང་ སྤྱོད་པ་ ཡིན་པ འི་ ཕྱིར་ ལ །_ དེ་དག་ ནི་ ཞི་གནས་ ལྷག་པ་ ཡིན་པ འི་ ཕྱིར་ རོ །_། གཞན་ དུ་ ཡང་ འགྲོ་བ ར་ མི་ ནུས་ སོ་ ཞེས་ བྱ་བ་ ནི་ ས་འོག་ མར་ སྟེ །\n",
      "ཕྱིར་ མི་ འོང་བ་ རྣམས་ ཁྱད་པར་ དུ་ འགྲོ་བ་ ཡིན་པ འི་ ཕྱིར་ རོ །_། ཅིའི་ཕྱིར་ དུས་ཡུན་ རིང་མོ་ ཞིག་ ན་ ཆུས་ འཇིག་ ལ །_ དེ་བ ས་ ཀྱང་ ཆེས་ རིང་བ་ ན་ རླུང་ གིས་ འཇིག་ ཅེ་ ན །\n",
      "དེ འི་ ཕྱིར་ སེམས་ཅན་ དེ་དག་ གི་ སྙོམས་པར་ འཇུག་པ འི་ ཁྱད་པར་ ལ ས་ ཞེས་ བྱ་བ་ རྒྱས་པ ར་ སྨོས་ ཏེ །\n",
      "སྙོམས་པར་ འཇུག་པ འི་ ཁྱད་པར་ ནི་ བསམ་གཏན་ གཉིས་པ་ དང་ །_ གསུམ་པ འི་ ཁྱད་པར་ རོ །_། ལུས་ གནས་པ འི་ ཁྱད་པར་ ཞེས་ བྱ་བ་ ནི་ གསོན་པ འི་ རྒྱུན་ གྱི་ ཁྱད་པར་ ཏེ །\n",
      "དུས་ དེ་ཙམ་ གྱིས་ དེ་དག་ གི་ ཚེ་ཟད་པ ར་ འགྱུར་ རོ །_། འཇིག་པ་ དེ་དག་ ནི་ དྲུག་ཅུ་ རྩ་ བཞི ར་ འགྱུར་ ལ །_ འཇིག་པ་ རེ་རེ་ ཡང་ བསྐལ་པ་ ཆེན་པོ་ ཡིན་ ནོ །_། དེ་ཉིད་ ཀྱི་ ཕྱིར་ དེ་ལྟར་ བཤད་ ན་\n",
      "བརྟགས་པ འི་ བཤད་པ་ ཞེས་ བྱ་བ་ རྒྱས་པ ར་ སྨོས་ སོ །_།\n",
      "སློབ་དཔོན་ གང་བ་ སྤེལ་བ ས་ མཛད་པ་ ཆོས་ མངོན་པ་ མཛོད་ ཀྱི་ འགྲེལ་བཤད་ མཚན་ཉིད་ ཀྱི་ རྗེས་སུ་ འབྲང་བ་ ཞེས་ བྱ་བ་ ལ ས །_ འཇིག་རྟེན་ བསྟན་པ་ ཞེས་ བྱ་བ་ མཛོད་ ཀྱི་ གནས་ གསུམ་པ འི་ འགྲེལ་བཤད་ དོ །།_།།"
     ]
    }
   ],
   "source": [
    "!tail {tokenized_corpus_fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2990486\n"
     ]
    }
   ],
   "source": [
    "!cat {corpus_fn} | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train sentencepie model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'classical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp(input_fn, model_name, model_type='unigram'):\n",
    "    model_prefix = f'{model_name}-{model_type}'\n",
    "    \n",
    "    if (models_path/f'{model_prefix}.model').is_file():\n",
    "        return models_path/model_prefix\n",
    "\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "    f'--input={input_fn} \\\n",
    "      --model_prefix={model_prefix} \\\n",
    "      --vocab_size=30000 \\\n",
    "      --model_type={model_type} \\\n",
    "      --character_coverage=0.9998 \\\n",
    "      --unk_id=0 \\\n",
    "      --pad_id=-1 \\\n",
    "      --bos_id=-1 \\\n",
    "      --eos_id=-1'\n",
    "    )\n",
    "\n",
    "    os.system(f'mv {model_prefix}.* {models_path}')\n",
    "    return models_path/model_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word base model - tokenized by botok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ཡིད་', '▁དགའ་བ', '▁འི་', '▁ལུས་', '▁ཤིན་ཏུ་', '▁སྦྱངས་པ', '▁ར་', '▁འགྱུར་', '▁རོ་', '▁ཞེས་', '▁འབྱུང་བ', '▁འི་', '▁ཕྱིར་', '▁རོ', '▁།_།', '▁ལུས་', '▁མཉེན་པ', '▁ར་', '▁བྱེད་པ་', '▁ཞེས་', '▁བྱ་བ་', '▁ནི་ལ', '▁ས་', '▁སུ་', '▁རུང་བ', '▁ར་', '▁བྱེད་པ', '▁འོ', '▁།_།']\n",
      "[123, 373, 5, 106, 160, 2156, 6, 45, 162, 15, 199, 5, 29, 44, 4, 106, 6939, 6, 79, 15, 22, 1301, 9, 40, 726, 6, 104, 16, 4]\n"
     ]
    }
   ],
   "source": [
    "model_path = get_sp(tokenized_corpus_fn, model_name, model_type='word')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f'{model_path}.model')\n",
    "\n",
    "print(sp.encode_as_pieces('ཡིད་ དགའ་བ འི་ ལུས་ ཤིན་ཏུ་ སྦྱངས་པ ར་ འགྱུར་ རོ་ ཞེས་ འབྱུང་བ འི་ ཕྱིར་ རོ །_། ལུས་ མཉེན་པ ར་ བྱེད་པ་ ཞེས་ བྱ་བ་ ནི་ལ ས་ སུ་ རུང་བ ར་ བྱེད་པ འོ །_།'))\n",
    "print(sp.encode_as_ids('ཡིད་ དགའ་བ འི་ ལུས་ ཤིན་ཏུ་ སྦྱངས་པ ར་ འགྱུར་ རོ་ ཞེས་ འབྱུང་བ འི་ ཕྱིར་ རོ །_། ལུས་ མཉེན་པ ར་ བྱེད་པ་ ཞེས་ བྱ་བ་ ནི་ལ ས་ སུ་ རུང་བ ར་ བྱེད་པ འོ །_།'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ཀྱང་ཁ་ལོ་སྒྱུར་བ་ལ་གསུངས་པ།▁ཁ་ལོ་སྒྱུར་བ་མི་སྐྱ་བོ་རྦབ་རྦབ་པོ་རིད་པ་ཉམ་ཆུང་བ་དབང་པོ་ཉམས་པ།▁སྐྱེ་བོ་མང་པོའི་མིག་གིས་ཀྱང་ལྟ་མི་ཕོད་པ་འདི་ཅི་ཡིན།']\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('ཀྱང་ཁ་ལོ་སྒྱུར་བ་ལ་གསུངས་པ། ཁ་ལོ་སྒྱུར་བ་མི་སྐྱ་བོ་རྦབ་རྦབ་པོ་རིད་པ་ཉམ་ཆུང་བ་དབང་པོ་ཉམས་པ། སྐྱེ་བོ་མང་པོའི་མིག་གིས་ཀྱང་ལྟ་མི་ཕོད་པ་འདི་ཅི་ཡིན།'))\n",
    "print(sp.encode_as_ids('ཀྱང་ཁ་ལོ་སྒྱུར་བ་ལ་གསུངས་པ། ཁ་ལོ་སྒྱུར་བ་མི་སྐྱ་བོ་རྦབ་རྦབ་པོ་རིད་པ་ཉམ་ཆུང་བ་དབང་པོ་ཉམས་པ། སྐྱེ་བོ་མང་པོའི་མིག་གིས་ཀྱང་ལྟ་མི་ཕོད་པ་འདི་ཅི་ཡིན།'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ཀྱང་', 'ཁ་ལོ་སྒྱུར་བ་', 'ལ་', 'གསུངས་པ།', '▁', 'ཁ་ལོ་སྒྱུར་བ་', 'མི་', 'སྐྱ་', 'བོ་', 'རྦ', 'བ་', 'རྦ', 'བ་', 'པོ་', 'རིད་', 'པ་', 'ཉམ་ཆུང་བ', '་', 'དབང་པོ་', 'ཉམས་པ', '།', '▁', 'སྐྱེ་བོ་མང་པོའི་', 'མིག་གིས་', 'ཀྱང་', 'ལྟ་', 'མི་ཕོད', '་པ་', 'འདི་', 'ཅི་', 'ཡིན།']\n",
      "[45, 13335, 62, 1577, 25, 13335, 167, 8113, 1608, 7613, 132, 7613, 132, 408, 21432, 91, 13213, 21, 1181, 5271, 38, 25, 29883, 7110, 324, 2867, 19799, 4799, 269, 914, 533]\n"
     ]
    }
   ],
   "source": [
    "model_path = get_sp(corpus_fn, model_name)\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f'{model_path}.model')\n",
    "\n",
    "print(sp.encode_as_pieces('ཀྱང་ཁ་ལོ་སྒྱུར་བ་ལ་གསུངས་པ། ཁ་ལོ་སྒྱུར་བ་མི་སྐྱ་བོ་རྦབ་རྦབ་པོ་རིད་པ་ཉམ་ཆུང་བ་དབང་པོ་ཉམས་པ། སྐྱེ་བོ་མང་པོའི་མིག་གིས་ཀྱང་ལྟ་མི་ཕོད་པ་འདི་ཅི་ཡིན།'))\n",
    "print(sp.encode_as_ids('ཀྱང་ཁ་ལོ་སྒྱུར་བ་ལ་གསུངས་པ། ཁ་ལོ་སྒྱུར་བ་མི་སྐྱ་བོ་རྦབ་རྦབ་པོ་རིད་པ་ཉམ་ཆུང་བ་དབང་པོ་ཉམས་པ། སྐྱེ་བོ་མང་པོའི་མིག་གིས་ཀྱང་ལྟ་མི་ཕོད་པ་འདི་ཅི་ཡིན།'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'ཁ་ལོ་སྒྱུར་བ་', 'ང་', 'ཡང་', 'ནད་ཀྱི་', 'ཆོས་དང་', 'ནད་', 'ཀྱི་ཆོས་ཉིད་', 'ལས་མ་འདས་', 'སམ།', '▁ལྷ་', 'ཡང་', 'སྙུན་', 'གྱི་ཆོས་', 'དང་', 'སྙུན་', 'གྱི་', 'ཆོས་ཉིད་', 'ལས་མ་འདས་', 'སོ།', '▁།']\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('ཁ་ལོ་སྒྱུར་བ་ང་ཡང་ནད་ཀྱི་ཆོས་དང་ནད་ཀྱི་ཆོས་ཉིད་ལས་མ་འདས་སམ། ལྷ་ཡང་སྙུན་གྱི་ཆོས་དང་སྙུན་གྱི་ཆོས་ཉིད་ལས་མ་འདས་སོ། །'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE (Byte pair encoding) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ཀྱང་', 'ཁ་ལོ་སྒྱུར་བ་', 'ལ་', 'གསུངས་པ།', '▁ཁ་ལོ་', 'སྒྱུར་བ་', 'མི་', 'སྐྱ', '་བོ་', 'རྦ', 'བ་', 'རྦ', 'བ་', 'པོ་', 'ར', 'ིད་པ་', 'ཉམ་', 'ཆུང་བ་', 'དབང་པོ་', 'ཉམས་', 'པ།', '▁སྐྱེ་བོ་', 'མང་པོའི་', 'མིག་གིས་', 'ཀྱང་', 'ལྟ་', 'མི་', 'ཕོད་པ་', 'འདི་ཅི་', 'ཡིན།']\n",
      "[234, 26573, 15, 2935, 9094, 11166, 190, 303, 2948, 1136, 17, 1136, 17, 68, 29935, 6281, 8008, 6292, 1576, 1690, 523, 1722, 7866, 7788, 361, 392, 190, 24202, 19458, 898]\n"
     ]
    }
   ],
   "source": [
    "model_path = get_sp(corpus_fn, model_name, model_type='bpe')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f'{model_path}.model')\n",
    "\n",
    "print(sp.encode_as_pieces('ཀྱང་ཁ་ལོ་སྒྱུར་བ་ལ་གསུངས་པ། ཁ་ལོ་སྒྱུར་བ་མི་སྐྱ་བོ་རྦབ་རྦབ་པོ་རིད་པ་ཉམ་ཆུང་བ་དབང་པོ་ཉམས་པ། སྐྱེ་བོ་མང་པོའི་མིག་གིས་ཀྱང་ལྟ་མི་ཕོད་པ་འདི་ཅི་ཡིན།'))\n",
    "print(sp.encode_as_ids('ཀྱང་ཁ་ལོ་སྒྱུར་བ་ལ་གསུངས་པ། ཁ་ལོ་སྒྱུར་བ་མི་སྐྱ་བོ་རྦབ་རྦབ་པོ་རིད་པ་ཉམ་ཆུང་བ་དབང་པོ་ཉམས་པ། སྐྱེ་བོ་མང་པོའི་མིག་གིས་ཀྱང་ལྟ་མི་ཕོད་པ་འདི་ཅི་ཡིན།'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁ཁ་ལོ་', 'སྒྱུར་བ་', 'ང་ཡང་', 'ནད་ཀྱི་', 'ཆོས་དང་', 'ནད་ཀྱི་', 'ཆོས་ཉིད་', 'ལས་མ་འདས་', 'སམ།', '▁ལྷ་', 'ཡང་', 'སྙུན་', 'གྱི་', 'ཆོས་དང་', 'སྙུན་', 'གྱི་', 'ཆོས་ཉིད་', 'ལས་མ་འདས་', 'སོ།', '▁།']\n"
     ]
    }
   ],
   "source": [
    "print(sp.encode_as_pieces('ཁ་ལོ་སྒྱུར་བ་ང་ཡང་ནད་ཀྱི་ཆོས་དང་ནད་ཀྱི་ཆོས་ཉིད་ལས་མ་འདས་སམ། ལྷ་ཡང་སྙུན་གྱི་ཆོས་དང་སྙུན་གྱི་ཆོས་ཉིད་ལས་མ་འདས་སོ། །'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'ཀ', 'ྱ', 'ང', '་', 'ཁ', '་', 'ལ', 'ོ', '་', 'ས', 'ྒ', 'ྱ', 'ུ', 'ར', '་', 'བ', '་', 'ལ', '་', 'ག', 'ས', 'ུ', 'ང', 'ས', '་', 'པ', '།', '▁', 'ཁ', '་', 'ལ', 'ོ', '་', 'ས', 'ྒ', 'ྱ', 'ུ', 'ར', '་', 'བ', '་', 'མ', 'ི', '་', 'ས', 'ྐ', 'ྱ', '་', 'བ', 'ོ', '་', 'ར', 'ྦ', 'བ', '་', 'ར', 'ྦ', 'བ', '་', 'པ', 'ོ', '་', 'ར', 'ི', 'ད', '་', 'པ', '་', 'ཉ', 'མ', '་', 'ཆ', 'ུ', 'ང', '་', 'བ', '་', 'ད', 'བ', 'ང', '་', 'པ', 'ོ', '་', 'ཉ', 'མ', 'ས', '་', 'པ', '།', '▁', 'ས', 'ྐ', 'ྱ', 'ེ', '་', 'བ', 'ོ', '་', 'མ', 'ང', '་', 'པ', 'ོ', 'འ', 'ི', '་', 'མ', 'ི', 'ག', '་', 'ག', 'ི', 'ས', '་', 'ཀ', 'ྱ', 'ང', '་', 'ལ', 'ྟ', '་', 'མ', 'ི', '་', 'ཕ', 'ོ', 'ད', '་', 'པ', '་', 'འ', 'ད', 'ི', '་', 'ཅ', 'ི', '་', 'ཡ', 'ི', 'ན', '།']\n",
      "[5, 27, 17, 19, 4, 37, 4, 22, 11, 4, 6, 35, 17, 21, 12, 4, 9, 4, 22, 4, 10, 6, 21, 19, 6, 4, 15, 16, 5, 37, 4, 22, 11, 4, 6, 35, 17, 21, 12, 4, 9, 4, 18, 7, 4, 6, 39, 17, 4, 9, 11, 4, 12, 51, 9, 4, 12, 51, 9, 4, 15, 11, 4, 12, 7, 8, 4, 15, 4, 32, 18, 4, 30, 21, 19, 4, 9, 4, 8, 9, 19, 4, 15, 11, 4, 32, 18, 6, 4, 15, 16, 5, 6, 39, 17, 14, 4, 9, 11, 4, 18, 19, 4, 15, 11, 20, 7, 4, 18, 7, 10, 4, 10, 7, 6, 4, 27, 17, 19, 4, 22, 26, 4, 18, 7, 4, 31, 11, 8, 4, 15, 4, 20, 8, 7, 4, 28, 7, 4, 23, 7, 13, 16]\n"
     ]
    }
   ],
   "source": [
    "model_path = get_sp(corpus_fn, model_name, model_type='char')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f'{model_path}.model')\n",
    "\n",
    "print(sp.encode_as_pieces('ཀྱང་ཁ་ལོ་སྒྱུར་བ་ལ་གསུངས་པ། ཁ་ལོ་སྒྱུར་བ་མི་སྐྱ་བོ་རྦབ་རྦབ་པོ་རིད་པ་ཉམ་ཆུང་བ་དབང་པོ་ཉམས་པ། སྐྱེ་བོ་མང་པོའི་མིག་གིས་ཀྱང་ལྟ་མི་ཕོད་པ་འདི་ཅི་ཡིན།'))\n",
    "print(sp.encode_as_ids('ཀྱང་ཁ་ལོ་སྒྱུར་བ་ལ་གསུངས་པ། ཁ་ལོ་སྒྱུར་བ་མི་སྐྱ་བོ་རྦབ་རྦབ་པོ་རིད་པ་ཉམ་ཆུང་བ་དབང་པོ་ཉམས་པ། སྐྱེ་བོ་མང་པོའི་མིག་གིས་ཀྱང་ལྟ་མི་ཕོད་པ་འདི་ཅི་ཡིན།'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbd-colab\n",
    "\n",
    "from nbd_colab import *\n",
    "\n",
    "drive_setup()\n",
    "home_dir()\n",
    "\n",
    "repo_name = 'bonltk'\n",
    "change_dir(f'/content/drive/My Drive/Notebooks/Esukhia/{repo_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
